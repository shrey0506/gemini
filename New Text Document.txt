import os
import langchain
import langgraph
import requests
from flask import Flask, request, jsonify
from langchain.llms import OpenAI, HuggingFaceHub
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# Initialize Flask app (Controller in MVC)
app = Flask(__name__)

# Load environment variables for API keys
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HUGGINGFACEHUB_API_KEY = os.getenv("HUGGINGFACEHUB_API_KEY")

# Define available models (Model in MVC)
MODELS = {
    "openai": OpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY),
    "huggingface": HuggingFaceHub(repo_id="facebook/bart-large-cnn", huggingfacehub_api_token=HUGGINGFACEHUB_API_KEY)
}

VECTOR_DB = None

def load_and_index_document(file_url):
    """Loads document from GCS signed URL and indexes it."""
    global VECTOR_DB
    response = requests.get(file_url)
    file_path = "/tmp/uploaded_file"
    
    with open(file_path, "wb") as f:
        f.write(response.content)
    
    if file_url.endswith(".pdf"):
        loader = PyPDFLoader(file_path)
    elif file_url.endswith(".docx"):
        loader = UnstructuredWordDocumentLoader(file_path)
    elif file_url.endswith(".pptx"):
        loader = UnstructuredPowerPointLoader(file_path)
    else:
        return "Unsupported file format"
    
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
    
    VECTOR_DB = FAISS.from_documents(chunks, OpenAIEmbeddings())
    return "Document successfully indexed"

def create_summary_chain(model_name):
    """Creates an LLM chain for text summarization."""
    if model_name not in MODELS:
        raise ValueError("Invalid model selected")
    
    prompt = PromptTemplate(template="Summarize the following text:\n{text}", input_variables=["text"])
    chain = LLMChain(llm=MODELS[model_name], prompt=prompt)
    return chain

def query_document(query):
    """Retrieves the most relevant text chunk from the indexed document and answers the query."""
    if VECTOR_DB is None:
        return "No document indexed yet. Please upload a document first."
    
    docs = VECTOR_DB.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])
    prompt = f"Answer the question based on the document:\nContext:\n{context}\n\nQuestion: {query}\nAnswer:"
    model = MODELS["openai"]
    return model(prompt)

@app.route("/upload", methods=["POST"])
def upload_document():
    """Handles document upload and indexing."""
    data = request.json
    file_url = data.get("file_url")
    
    if not file_url:
        return jsonify({"error": "Missing file URL"}), 400
    
    message = load_and_index_document(file_url)
    return jsonify({"message": message})

@app.route("/ask", methods=["POST"])
def ask_question():
    """Handles querying the uploaded document."""
    data = request.json
    query = data.get("query", "")
    
    answer = query_document(query)
    return jsonify({"answer": answer})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)



!pip install pdfplumber pandas

import pdfplumber
import pandas as pd
import numpy as np

# Define the PDF file path
pdf_file = 'onepageisleofman.pdf'

# Threshold for grouping lines as a single row (adjust if needed)
y_threshold = 5

# Column boundaries (based on your specified ranges)
columns = [0, 50, 150, 370, 490, 600, 720, 820]

# Function to group text by row and stitch multiline cells
def extract_and_stitch(pdf_file, columns, y_threshold):
    rows = []

    # Open the PDF file
    with pdfplumber.open(pdf_file) as pdf:
        # Iterate through each page
        for page in pdf.pages:
            # Extract text with positional data
            words = page.extract_words()
            
            # Sort words by y-coordinate (top to bottom)
            words.sort(key=lambda w: w['top'])
            
            # Group words by row using y-coordinates
            current_row = []
            last_y = None
            
            for word in words:
                # Check if the word starts a new row
                if last_y is None or abs(word['top'] - last_y) > y_threshold:
                    # If a new row starts, save the current row
                    if current_row:
                        rows.append(current_row)
                    # Start a new row
                    current_row = [word]
                else:
                    # Continue in the same row
                    current_row.append(word)
                
                # Update last_y for the next iteration
                last_y = word['top']
            
            # Append the last accumulated row
            if current_row:
                rows.append(current_row)

    # Convert the rows into a DataFrame
    data = []
    for row in rows:
        # Create a blank row for each column
        row_data = [''] * (len(columns) - 1)
        
        # Place each word into the correct column
        for word in row:
            x_mid = (word['x0'] + word['x1']) / 2
            for i, (col_start, col_end) in enumerate(zip(columns[:-1], columns[1:])):
                if col_start <= x_mid < col_end:
                    row_data[i] += ' ' + word['text']
                    break
        
        # Strip leading/trailing spaces
        row_data = [cell.strip() for cell in row_data]
        data.append(row_data)

    # Create DataFrame and save to CSV
    df = pd.DataFrame(data, columns=[f'Column {i+1}' for i in range(len(columns)-1)])
    df.to_csv('stitched_pdfplumber_table.csv', index=False)
    return df

# Run the extraction and stitching process
stitched_df = extract_and_stitch(pdf_file, columns, y_threshold)

# Display the stitched DataFrame
print("\nStitched Table Preview:")
print(stitched_df.head())



pattern = re.compile(r'\d{3,4}\s+([A-Z][A-Z\s\-\']+)(?:\n|$)', re.MULTILINE)

# Finding all matches
charities = pattern.findall(pdf_text)
