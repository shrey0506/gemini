use# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

import re
import pandas as pd


# Function to process the text and extract relevant information
def process_text(text):
    # Split the text into individual charity entries
    charity_entries = re.split(r'(?m)^\d{3,4}\s', text)[1:]
    charity_numbers = re.findall(r'(?m)^\d{3,4}', text)

    # Initialize lists for each column
    numbers = []
    charities = []
    objects = []
    addresses = []
    emails = []
    websites = []
    dates = []

    # Loop through each charity entry to extract information
    for entry in charity_entries:
        # Extract charity number
        number_match = re.search(r'^\d{3,4}', entry)
        if number_match:
            numbers.append(number_match.group(0))
        else:
            numbers.append('')  # Fill with empty string if no match
        
        # Extract charity name
        charity_match = re.search(r'^\d{3,4}\s([A-Z][A-Z0-9\s\-\(\)]*[A-Z\)])', entry)
        if charity_match:
            charities.append(charity_match.group(1).strip())
        else:
            charities.append('')  # Fill with empty string if no match
        
        # Extract objects (content before the address block)
        object_match = re.search(r'^(.*?)(?=\n[^\n]*IM\d\s\d[A-Z]{2})', entry, re.DOTALL)
        if object_match:
            objects.append(object_match.group(1).strip())
        else:
            objects.append('')  # Fill with empty string if no match
        
        # Extract address (block ending with postal code)
        address_match = re.search(r'([^\n]*\n)*[^\n]*IM\d\s\d[A-Z]{2}', entry, re.DOTALL)
        if address_match:
            addresses.append(address_match.group(0).strip())
        else:
            addresses.append('')  # Fill with empty string if no match
        
        # Extract email
        email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', entry)
        if email_match:
            emails.append(email_match.group(0))
        else:
            emails.append('')  # Fill with empty string if no match
        
        # Extract website
        website_match = re.search(r'(www\.[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}|[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:\.[a-zA-Z]{2,})?)', entry)
        if website_match:
            websites.append(website_match.group(0))
        else:
            websites.append('')  # Fill with empty string if no match
        
        # Extract date
        date_match = re.search(r'\d{1,2} [A-Za-z]{3} \d{4}|\d{4}', entry)
        if date_match:
            dates.append(date_match.group(0))
        else:
            dates.append('')  # Fill with empty string if no match

    # Create a DataFrame
    df = pd.DataFrame({
        'No.': numbers,
        'Charity': charities,
        'Objects': objects,
        'Correspondence address': addresses,
        'Email address': emails,
        'Website': websites,
        'Date Registered': dates
    })

    return df

# Process the text and create the DataFrame
df = process_text(text)

# Display the DataFrame
df.head()





Here is your updated chunked record linkage code, optimized to store results in SQLite instead of keeping everything in memory. This will prevent the kernel from crashing due to memory limits.


---

Updated Code: Uses SQLite for Storing Matches

import gc
import sqlite3
import pandas as pd
import recordlinkage

# Function to match and compare
def match_and_compare(a_chunk, banking):
    indexer = recordlinkage.Index()
    indexer.block(left_on='COUNTRY_OF_REG', right_on='COUNTRY_OF_REGISTRATION_eu')
    candidate_links = indexer.index(a_chunk, banking)

    compare = recordlinkage.Compare()
    compare.string('LEGAL_NAME', 'CompanyName', label='bank_Name_match')
    compare.exact('COUNTRY_OF_REG', 'COUNTRY_OF_REGISTRATION_eu', label='countryeu_match')
    compare.exact('COUNTRY_OF_REG', 'COUNTRY_NAME_Swift', label='countrySwift_match')
    compare.exact('COUNTRY_OF_REG', 'COUNTRY_gleif', label='countryGleif_match')
    compare.exact('REG_ADDRESS_POST_CODE', 'POSTCODE_gleif', label='postcodeGleif_match')
    compare.exact('REG_ADDRESS_POST_CODE', 'POSTCODE_eu', label='postcodeeu_match')
    compare.exact('REG_ADDRESS_POST_CODE', 'POSTCODE_Swift', label='postcodeSwift_match')

    features_chunk = compare.compute(candidate_links, a_chunk, banking)
    return features_chunk

# Function to process chunks and store in SQLite
def process_chunks(a, banking, chunk_size=5000, db_name="matches.db"):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    # Create table if not exists
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS matches (
            bank_Name_match REAL,
            countryeu_match INTEGER,
            countrySwift_match INTEGER,
            countryGleif_match INTEGER,
            postcodeGleif_match INTEGER,
            postcodeeu_match INTEGER,
            postcodeSwift_match INTEGER
        )
    """)
    conn.commit()

    for start in range(0, len(a), chunk_size):
        end = min(start + chunk_size, len(a))
        a_chunk = a.iloc[start:end]

        features_chunk = match_and_compare(a_chunk, banking)

        # Append to SQLite database
        features_chunk.to_sql("matches", conn, if_exists="append", index=False)

        # Free memory
        del a_chunk, features_chunk
        gc.collect()

    conn.close()

# Run the chunked processing
process_chunks(a, banking, chunk_size=5000)


---

Key Improvements

✅ Uses SQLite instead of keeping results in memory
✅ Writes in chunks, preventing memory buildup
✅ Efficient candidate reduction with blocking (COUNTRY_OF_REG)
✅ Garbage collection (gc.collect()) to free memory

This will prevent kernel crashes since results are stored in a lightweight SQLite database.

Would you like me to add indexing in SQLite to make queries faster?

If the column names are dynamic (not fixed), we need to:

1. Dynamically get column names from both dataframes


2. Ensure correct column mappings


3. Handle missing columns gracefully




---

Updated Code: Dynamic Column Handling

This code will automatically adjust to the available columns and avoid crashes due to missing labels.

import gc
import sqlite3
import pandas as pd
import recordlinkage

def match_and_compare(a_chunk, banking):
    indexer = recordlinkage.Index()

    # Dynamically find common columns for blocking
    common_cols = list(set(a_chunk.columns) & set(banking.columns))
    
    if 'COUNTRY_OF_REG' in common_cols and 'COUNTRY_OF_REGISTRATION_eu' in common_cols:
        indexer.block(left_on='COUNTRY_OF_REG', right_on='COUNTRY_OF_REGISTRATION_eu')
    else:
        print("Warning: Blocking columns not found. Proceeding without blocking.")
        return pd.DataFrame()  # Return empty DataFrame if essential columns are missing

    candidate_links = indexer.index(a_chunk, banking)
    compare = recordlinkage.Compare()

    # Dynamically add comparison conditions only if columns exist
    column_mappings = [
        ('LEGAL_NAME', 'CompanyName', 'bank_Name_match'),
        ('COUNTRY_OF_REG', 'COUNTRY_OF_REGISTRATION_eu', 'countryeu_match'),
        ('COUNTRY_OF_REG', 'COUNTRY_NAME_Swift', 'countrySwift_match'),
        ('COUNTRY_OF_REG', 'COUNTRY_gleif', 'countryGleif_match'),
        ('REG_ADDRESS_POST_CODE', 'POSTCODE_gleif', 'postcodeGleif_match'),
        ('REG_ADDRESS_POST_CODE', 'POSTCODE_eu', 'postcodeeu_match'),
        ('REG_ADDRESS_POST_CODE', 'POSTCODE_Swift', 'postcodeSwift_match')
    ]

    for left_col, right_col, label in column_mappings:
        if left_col in a_chunk.columns and right_col in banking.columns:
            compare.exact(left_col, right_col, label=label)
        else:
            print(f"Skipping {label}: {left_col} or {right_col} not found.")

    # Compute only if there are valid comparisons
    if compare._comparisons:
        features_chunk = compare.compute(candidate_links, a_chunk, banking)
        return features_chunk
    else:
        print("Warning: No valid comparisons found.")
        return pd.DataFrame()

def process_chunks(a, banking, chunk_size=5000, db_name="matches.db"):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    # Dynamically create table based on available columns
    sample_chunk = match_and_compare(a.iloc[:1], banking)
    if sample_chunk.empty:
        print("Error: No matching columns found. Stopping process.")
        return

    create_table_query = f"""
        CREATE TABLE IF NOT EXISTS matches (
            {", ".join([f'"{col}" REAL' for col in sample_chunk.columns])}
        )
    """
    cursor.execute(create_table_query)
    conn.commit()

    for start in range(0, len(a), chunk_size):
        end = min(start + chunk_size, len(a))
        a_chunk = a.iloc[start:end]

        features_chunk = match_and_compare(a_chunk, banking)

        if not features_chunk.empty:
            features_chunk.to_sql("matches", conn, if_exists="append", index=False)

        del a_chunk, features_chunk
        gc.collect()

    conn.close()

# Run the chunked processing
process_chunks(a, banking, chunk_size=5000)


---

Key Fixes in This Version

✅ Dynamically selects columns instead of hardcoding
✅ Skips missing columns instead of crashing
✅ Automatically creates SQLite table based on available columns
✅ Prevents empty processing (checks before running comparisons)


---

Try This Code and Let Me Know If It Works!

If you still have issues, share the output of:

print(a.columns)
print(banking.columns)

so I can debug further.

